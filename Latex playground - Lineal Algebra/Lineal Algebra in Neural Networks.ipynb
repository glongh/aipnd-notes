{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lineal Algebra in Neural Networks\n",
    "\n",
    "The design of the Artificial Neural Network was inspired by the biological one. The neurons used in the artificial network below are essentially mathematical functions.\n",
    "\n",
    "Each network has:\n",
    "* Input neurons: which we refer to as the input layers of neurons.\n",
    "* Output neurons: which we refer to as the output layer of neurons.\n",
    "* Internal neurons: which we refer to as the hidden layer of neurons. Each neural network can have many hidden layers.\n",
    "\n",
    "![](images/simple_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of a simplified artificial neural network is compromise out of:\n",
    "* An input vector $\\vec{x} = [x_1 x_2 x_3 ... x_n]$\n",
    "* A hidden layer vector $\\vec{h} = [h_1 h_2 h_3 ... h_m]$\n",
    "* An output vector $\\vec{y} = [y_1 y_2 y_3 ... y_k]$\n",
    "\n",
    "Each element in the vectors is a mathematical argument\n",
    "There is no connection between the number of inputs, number of hidden neurons in the hidden layer or number of outputa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines connecting the different:\n",
    "* In practice, these lines symbolize a coefficient (a scalar) that is mathematically connecting one neuron to the next. These coefficients are called *__weights__*\n",
    "* The \"lines\" connect each neuron in a specific layer to *__all__* of neurons on the following. For example, in out example, you can see how each neuron is the hidden layer is connected to a neuron in the output one.\n",
    "\n",
    "Since there are so many *__weights__* connecting one layer to the next, we mathematically organize those coefficients in a matrix, denoted as the *__weight matrix__*\n",
    "\n",
    "![](images/weight_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_k$ is the weight matrix k\n",
    "\n",
    "$w^k_{ij}$ is the ij element of the weight matrix k\n",
    "\n",
    "\n",
    "![](images/activation_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with neural networks we have 2 primary phases:\n",
    "* Training\n",
    "* Evaluation\n",
    "\n",
    "During the training phase, we take the data set (also called training set), which include many pairs of inputs and their corresponding targets (outputs). Our goal is to find a set of weights that would best map the inputs to the desired outputs.\n",
    "\n",
    "In the evaluation phase, we use the network that was created in the training phase, apply out new inputs and expect to obtain the desired otputs.\n",
    "\n",
    "The training phase will include two steps:\n",
    "* Feedforward\n",
    "* Backpropagation\n",
    "\n",
    "We will repeat these steps as many times as we need until we decided that our system has reached the best set of weights, giving us the best possible outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Feedforward Process - Finding $\\vec{h}$ - STEP 1\n",
    "\n",
    "In this section we will look closely at the math behind the feedforward process. With the use of basic Linear Algebra tools, these calculations are pretty simple!\n",
    "Assuming that we have a single hidden layer, we will need two steps in our calculations. The first will be calculating the value of the hidden states and the latter will be calculating the value of the outputs.\n",
    "\n",
    "![](images/finding_h.png)\n",
    "\n",
    "Both the hidden layer and the output layer are displayed as vectors, as they are both represented by more than a single neuron.\n",
    "\n",
    "![](images/step1_finding_h.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector $\\vec{h}$ of the hidden layer will be caltulated by multiplying the input vecrtor with the weight matrix $W^1$ the following way:\n",
    "\n",
    "![](images/finding_h_simple.png)\n",
    "\n",
    "Using vector by matrix multiplication, we can look at this computation the following way:\n",
    "\n",
    "![](images/finding_h_matrix_multi.png)\n",
    "\n",
    "After finding $\\vec{h^1}$ we need an activation function, the symbol we use for the activation function is the greek letter phi: $\\phi$\n",
    "\n",
    "This activation function finalizes the computation of the hidden layer's values.\n",
    "\n",
    "We can use the following two equations to express the final hidden vector $\\vec{h^1}$\n",
    "\n",
    "$\\vec{h} = \\phi(\\vec{x}W^1)$\n",
    "\n",
    "or \n",
    "\n",
    "$\\vec{h} = \\phi(\\vec{h^1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $W_{ij}$ represents the weight component in the weight matrix, connectingneuron i from the input to neuron j in the hidden layer, we can also write these calculations using a *__linear combination__*\n",
    "(in this example we have n inputs and only 3 hidden neurons)\n",
    "\n",
    "$h_1 = \\phi(x_1W_11 + x_2W_21 + ... x_nW_n1)$\n",
    "\n",
    "$h_2 = \\phi(x_1W_12 + x_2W_22 + ... x_nW_n2)$\n",
    "\n",
    "$h_3 = \\phi(x_1W_13 + x_2W_23 + ... x_nW_n3)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Feedforward Process - Finding $\\vec{y}$ - STEP 2\n",
    "\n",
    "The process of calculating the output vector is mathematically similar to that of calculating the vector of the hidden layer. We use, again, a vector by matrix multiplication. The vector is the newly calculated hidden layer and the matrix is the one connecting the hidden layer to the output.\n",
    "\n",
    "<img src=\"images/feedforward_step2.png\" height=300 width=750/>\n",
    "\n",
    "Essentially, each new layer in an neural network is calculated by a vector by matrix multiplication, where the vector represents the inputs to the new layer and the matrix is the one connecting these new inputs to the next layer.\n",
    "\n",
    "ie: The input vector is $\\vec{h}$ and the matrix is $W^2$, therefore $\\vec{y} = \\vec{h}W^2$\n",
    "\n",
    "$\\begin{bmatrix}y_1&y_2\\end{bmatrix} = \\begin{bmatrix}h_1 & h_2 & h_3\\end{bmatrix}\\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22}\\\\ w_{31} & w_{32} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/feedforward_step2_a.png)\n",
    "![](images/feedforward_step2_b.png)\n",
    "![](images/feedforward_step2_c.png)\n",
    "![](images/feedforward_step2_d.png)\n",
    "![](images/feedforward_step2_e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
