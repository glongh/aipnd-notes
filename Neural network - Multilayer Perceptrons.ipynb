{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptrons\n",
    "\n",
    "\n",
    "![](images/deep_network_01.png)\n",
    "\n",
    "Calculating the output of this network is the same as before. Except that now the activation of the hidden layer are used as the input to the output layer.\n",
    "\n",
    "![](images/deep_network_02.png)\n",
    "\n",
    "The input is the same as before, is the sum of it's weights times the input values plus some bias term.\n",
    "\n",
    "![](images/deep_network_03.png)\n",
    "\n",
    "and as before, you use an activation function such a sigmoid to calculate the output of the hidden layer.\n",
    "\n",
    "![](images/deep_network_04.png)\n",
    "\n",
    "The hidden layer activations are passed to the output layer through the second set of weights, and again using an activation function to get the output of the network. \n",
    "\n",
    "![](images/deep_network_05.png)\n",
    "\n",
    "stacking more and more layers like this, helps the network learn more complex patterns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the hidden layer\n",
    "\n",
    "![](images/deep_network_hidden_1.png)\n",
    "\n",
    "The lines indicating the weights leading to $h_1$ have been colorader differently from those leading to $h_2$\n",
    "\n",
    "Now to index the weights, we take the input unit number for the $i$ and the hidden unit number for the \n",
    "$j$ That gives us\n",
    "\n",
    "$w_{11}$\n",
    "\n",
    "for the weight leading from $x_1$ to $h_1$ and\n",
    "\n",
    "$w_{12}$\n",
    "\n",
    "for the weight leading from $x_1$ to $h_2$.\n",
    "\n",
    "The following image includes all of the weights between the input layer and the hidden layer, labeled with their appropriate $w_{ij}$ indices:\n",
    "\n",
    "![](images/deep_network_hidden_1b.png)\n",
    "\n",
    "Before, we were able to write the weights as an array, indexed as $w1$\n",
    "\n",
    "But now, the weights need to be stored in a matrix, indexed as $w_{ij}$ Each row in the matrix will correspond to the weights leading out of a single input unit, and each column will correspond to the weights leading in to a single hidden unit. For our three input units and two hidden units, the weights matrix looks like this:\n",
    "\n",
    "![](images/deep_network_hidden_2.png)\n",
    "\n",
    "different weights in the network end up in the matrix.\n",
    "\n",
    "To initialize these weights in NumPy, we have to provide the shape of the matrix. If `features` is a 2D array containing the input data:\n",
    "\n",
    "```\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "```\n",
    "This creates a 2D array (i.e. a matrix) named weights_input_to_hidden with dimensions n_inputs by n_hidden. Remember how the input to a hidden unit is the sum of all the inputs multiplied by the hidden unit's weights. So for each hidden layer unit, $h_j$, we need to calculate the following:\n",
    "\n",
    "![](images/deep_network_hidden_3.png)\n",
    "\n",
    "To do that, we now need to use matrix multiplication. If your linear algebra is rusty, I suggest taking a look at the suggested resources in the prerequisites section. For this part though, you'll only need to know how to multiply a matrix with a vector.\n",
    "\n",
    "In this case, we're multiplying the inputs (a row vector here) by the weights. To do this, you take the dot (inner) product of the inputs with each column in the weights matrix. For example, to calculate the input to the first hidden unit, $j=1$,  you'd take the dot product of the inputs with the first column of the weights matrix, like so:\n",
    "\n",
    "![](images/deep_network_hidden_4.png)\n",
    "![](images/deep_network_hidden_5.png)\n",
    "\n",
    "And for the second hidden layer input, you calculate the dot product of the inputs with the second column. And so on and so forth.\n",
    "\n",
    "In NumPy, you can do this for all the inputs and all the outputs at once using np.dot\n",
    "\n",
    "```\n",
    "hidden_inputs = np.dot(inputs, weights_input_to_hidden)\n",
    "```\n",
    "\n",
    "You could also define your weights matrix such that it has dimensions n_hidden by n_inputs then multiply like so where the inputs form a column vector:\n",
    "\n",
    "![](images/deep_network_hidden_6.png)\n",
    "\n",
    "Note: The weight indices have changed in the above image and no longer match up with the labels used in the earlier diagrams. That's because, in matrix notation, the row index always precedes the column index, so it would be misleading to label them the way we did in the neural net diagram. Just keep in mind that this is the same weight matrix as before, but rotated so the first column is now the first row, and the second column is now the second row. If we were to use the labels from the earlier diagram, the weights would fit into the matrix in the following locations:\n",
    "\n",
    "![](images/deep_network_hidden_7.png)\n",
    "\n",
    "Remember, the above is not a correct view of the indices, but it uses the labels from the earlier neural net diagrams to show you where each weight ends up in the matrix.\n",
    "The important thing with matrix multiplication is that the dimensions match. For matrix multiplication to work, there has to be the same number of elements in the dot products. In the first example, there are three columns in the input vector, and three rows in the weights matrix. In the second example, there are three columns in the weights matrix and three rows in the input vector. If the dimensions don't match, you'll get this:\n",
    "\n",
    "```\n",
    "# Same weights and features as above, but swapped the order\n",
    "hidden_inputs = np.dot(weights_input_to_hidden, features)\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-11-1bfa0f615c45> in <module>()\n",
    "----> 1 hidden_in = np.dot(weights_input_to_hidden, X)\n",
    "\n",
    "ValueError: shapes (3,2) and (3,) not aligned: 2 (dim 1) != 3 (dim 0)\n",
    "```\n",
    "\n",
    "![](images/deep_network_hidden_8.png)\n",
    "\n",
    "The rule is that if you're multiplying an array from the left, the array must have the same number of elements as there are rows in the matrix. And if you're multiplying the matrix from the left, the number of columns in the matrix must equal the number of elements in the array on the right.\n",
    "\n",
    "#### Making a column vector\n",
    "You see above that sometimes you'll want a column vector, even though by default NumPy arrays work like row vectors. It's possible to get the transpose of an array like so `arr.T`, but for a 1D array, the transpose will return a row vector. Instead, use `arr[:,None]` to create a column vector:\n",
    "\n",
    "```\n",
    "\n",
    "print(features)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features.T)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features[:, None])\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "```\n",
    "\n",
    "Alternatively, you can create arrays with two dimensions. Then, you can use `arr.T` to get the column vector.\n",
    "\n",
    "```\n",
    "np.array(features, ndmin=2)\n",
    "> array([[ 0.49671415, -0.1382643 ,  0.64768854]])\n",
    "\n",
    "np.array(features, ndmin=2).T\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a forward pass through a 4x3x2 network, with sigmoid activation functions for both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[0.41492192 0.42604313 0.5002434 ]\n",
      "Output-layer Output:\n",
      "[0.49815196 0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
