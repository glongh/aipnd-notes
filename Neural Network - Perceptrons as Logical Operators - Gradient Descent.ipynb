{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons as Logical Operators\n",
    "Percerptrons have many applications, one of them is as a *__logical operators__*  AND, OR, NOT and XOR operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND perceptron\n",
    "![](images/nn_perceptron_and.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                  -2.0                    0          Yes\n",
      "      0          1                  -1.0                    0          Yes\n",
      "      1          0                  -1.0                    0          Yes\n",
      "      1          1                   0.0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1.0\n",
    "weight2 = 1.0\n",
    "bias = -2.0\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND to OR perceptron\n",
    "![](images/nn_perceptron_and-to-or.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                  -2.0                    0          Yes\n",
      "      0          1                   0.0                    1          Yes\n",
      "      1          0                   0.0                    1          Yes\n",
      "      1          1                   2.0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "\n",
    "# (1) Increase the weight\n",
    "weight1 = 2.0 \n",
    "weight2 = 2.0\n",
    "bias = -2.0\n",
    "\n",
    "# (2) Decreate the magnitude of the bias\n",
    "# weight1 = 1.0 \n",
    "# weight2 = 1.0\n",
    "# bias = -1.0\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, True, True, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOT perceptron\n",
    "Unlike the other perceptrons, the NOT operation only cares about one input. The operation returns a 0 if the input is 1 and a 1 if it's a 0. The other inputs to the perceptron are ignored.\n",
    "Following: setting the weights (`weight1`, `weight2`) and `bias` to the values that calculate the NOT operation on the second input and ignores the first input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                   0.0                    1          Yes\n",
      "      0          1                  -1.0                    0          Yes\n",
      "      1          0                   0.0                    1          Yes\n",
      "      1          1                  -1.0                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 0.0\n",
    "weight2 = -1.0\n",
    "bias = 0.0\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR Perceptron\n",
    "![](images/nn_perceptron_xor.png)\n",
    "![](images/nn_perceptron_nand.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Trick\n",
    "\n",
    "The misclassified point want the line to be closer until eventually clasify it correctly\n",
    "\n",
    "![](images/nn_perceptron_trick1.png)\n",
    "![](images/nn_perceptron_trick2.png)\n",
    "\n",
    "#### Mathematical trick that modify the equation of the line\n",
    "\n",
    "We want to move the line closer to that point:\n",
    "![](images/nn_perceptron_math_trick1.png)\n",
    "\n",
    "But doing so it will do a drastic move, probably affecting all other point\n",
    "We want to move the like in small steps using the learning rate.\n",
    "\n",
    "![](images/nn_perceptron_math_trick2.png)\n",
    "\n",
    "Same thing with the other point in the other direction.\n",
    "\n",
    "![](images/nn_perceptron_math_trick3.png)\n",
    "\n",
    "\n",
    "## Perceptron Algorithm\n",
    "\n",
    "Starting with : $Wx + b$\n",
    "\n",
    "Then we apply for each i from 1 to n:\n",
    "$w_i+ \\alpha x_i$\n",
    "\n",
    "Note: $\\alpha$ is the learning rate\n",
    "![](images/nn_perceptron_algo_pseudocode.png)\n",
    "\n",
    "Then we repeat this until no errors or for an arbitrary number.\n",
    "\n",
    "#### Perceptron steps:\n",
    "For a point with coordinates $(p,q)$, label $y$ and prediction given by the equation  $\\hat{y} = step(w_1x_1+w_2x_2+b)$:\n",
    "\n",
    "* If the point is correctly classified, do nothing.\n",
    "* If the point is classified positive, but it has a negative label, subtract $\\alpha p, \\alpha q$ and $\\alpha$ from $w_1, w_2 and b$ respectively.\n",
    "* if the point is classified negative, but it has a positive label, add $\\alpha p, \\alpha q, and \\alpha$ to $w_1, w_2, and b$ respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2\n",
      "0   1\n",
      "1   1\n",
      "2   1\n",
      "3   1\n",
      "4   1\n",
      "5   1\n",
      "6   1\n",
      "7   1\n",
      "8   1\n",
      "9   1\n",
      "10  1\n",
      "11  1\n",
      "12  1\n",
      "13  1\n",
      "14  1\n",
      "15  1\n",
      "16  1\n",
      "17  1\n",
      "18  1\n",
      "19  1\n",
      "20  1\n",
      "21  1\n",
      "22  1\n",
      "23  1\n",
      "24  1\n",
      "25  1\n",
      "26  1\n",
      "27  1\n",
      "28  1\n",
      "29  1\n",
      ".. ..\n",
      "70  0\n",
      "71  0\n",
      "72  0\n",
      "73  0\n",
      "74  0\n",
      "75  0\n",
      "76  0\n",
      "77  0\n",
      "78  0\n",
      "79  0\n",
      "80  0\n",
      "81  0\n",
      "82  0\n",
      "83  0\n",
      "84  0\n",
      "85  0\n",
      "86  0\n",
      "87  0\n",
      "88  0\n",
      "89  0\n",
      "90  0\n",
      "91  0\n",
      "92  0\n",
      "93  0\n",
      "94  0\n",
      "95  0\n",
      "96  0\n",
      "97  0\n",
      "98  0\n",
      "99  0\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "perceptron_data = pd.read_csv('data/perceptron_algorithm.csv', header=None)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b\n",
    "\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    W = np.array(np.random.rand(2,1))\n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n",
    "\n",
    "#var = trainPerceptronAlgorithm(perceptron_data.loc[:,0:1], perceptron_data.loc[:,2:2])\n",
    "#print(perceptron_data.loc[:,2:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/perceptron_algo_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Function:\n",
    "\n",
    "`Error Function = Distance`\n",
    "\n",
    "It has to be *__continuous__*, looking the direction in which reduce the error the most (Sum of penalties)\n",
    "\n",
    "![](images/perceptron_log_loss_error_function.png)\n",
    "\n",
    "To apply gradient descent:\n",
    "* The error function should be differentiable\n",
    "* The error function should be continuous\n",
    "\n",
    "![](images/discrete_continuous_0.png)\n",
    "\n",
    "![](images/discrete_continuous_1.png)\n",
    "\n",
    "![](images/discrete_continuous_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax function\n",
    "\n",
    "![](images/softmax_classification_problem.png)\n",
    "\n",
    "Linear function scores: $Z_1, ..., Z_n$\n",
    "\n",
    "P(class i) = $\\frac{e^{Z_i}}{e^{Z_1} +  ...  + e^{Z_n}}$\n",
    "\n",
    "`Softmax values for n=2 are the same as the sigmoid function values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09003057317038046, 0.24472847105479764, 0.6652409557748219]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    denominator = np.sum(np.exp(L))\n",
    "    return [np.exp(x)/denominator for x in L]\n",
    "\n",
    "softmax([5,6,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding \n",
    "Used for processing data\n",
    " \n",
    "![](images/one_hot_encoding.png)\n",
    " \n",
    "### Maximum Likelihood\n",
    "Picking the model that gives the existing labels the highest probability, by maximazing the probability we can pick the best possible model.\n",
    "\n",
    "![](images/maximum_likelihood_1.png)\n",
    "\n",
    "![](images/maximum_likelihood_2.png)\n",
    "\n",
    "![](images/maximum_likelihood_3.png)\n",
    "\n",
    "![](images/maximum_likelihood_4.png)\n",
    "\n",
    "\n",
    "### Cross-Entropy\n",
    "Point that are correctly classified will have small errors and points that are mis-classified will have large errors.\n",
    "\n",
    "![](images/cross-entropy_1.png)\n",
    "\n",
    "The cross entropy will tell us if a model is good or bad.\n",
    "The goal it to minimize the *__cross entropy__* \n",
    "\n",
    "![](images/cross-entropy_2.png)\n",
    "\n",
    "`The error function is the cross entropy.`\n",
    "\n",
    "![](images/cross-entropy_3.png)\n",
    "![](images/cross-entropy_4.png)\n",
    "\n",
    "The number is high `5.12` because the arrangement of gifts being given by the first set of numbers is very unlikely to happen from the probabilities given by the second set of numbers.\n",
    "\n",
    "![](images/cross-entropy_5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.828313737302301"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    return -np.sum([Y[i]*np.log(P[i]) + (1-Y[i])*np.log(1-P[i]) for i in range(len(P))])\n",
    "\n",
    "cross_entropy([1,0,1,1], [0.4,0.6,0.1,0.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Entropy\n",
    "\n",
    "![](images/multiple-class-entropy_1.png)\n",
    "\n",
    "![](images/multiple-class-entropy_2.png)\n",
    "\n",
    "*__`Cross-entropy is inversely proportional to the total probability of an outcome.`__*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "* Take your data\n",
    "* Pick a random model\n",
    "* Calculate the error\n",
    "* Minimize the error, and obtain a better model\n",
    "\n",
    "#### Calculating the Error Function\n",
    "\n",
    "![](images/error_function_0.png)\n",
    "\n",
    "![](images/error_function_1.png)\n",
    "\n",
    "*__This works because__*\n",
    "\n",
    "If the point is blue:\n",
    "![](images/error_function_2.png)\n",
    "\n",
    "\n",
    "If the point is red:\n",
    "![](images/error_function_3.png)\n",
    "\n",
    "\n",
    "_Not the sum, the average_: ($-\\frac{1}{m}$)\n",
    "![](images/error_function_4.png)\n",
    "\n",
    "#### Minimizing the error function\n",
    "We start with some ramdon weights, whicn will give us the predictions: $\\sigma(W_x+b)$, that also give us a prediction formula.\n",
    "Summands are also error functions for each point, so each point will give us a larger function if it's mis-classified and a smaller one if it's correctly classified. To minimize the error function we use gradient descent which give us a smaller error function $E(W',b')$ This will give rise to new weights \n",
    "\n",
    "![](images/gradient_descent_0.png)\n",
    "\n",
    "\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "![](images/gradient_descent_1.png)\n",
    "\n",
    "The gradient of E is given by the vector sum of the partial derivatives of E with respect to W1 and W2. This gradient actually tells us the direction we want to move it we want to increase the error function the most, but if we take the negative of gradient this will tell us how to decrease the error function the most. \n",
    "\n",
    "![](images/gradient_descent_2.png)\n",
    "\n",
    "#### Gradient calculation\n",
    "In order to minimize the error function, we need to take some derivatives.\n",
    "\n",
    "The sigmoid function has a nice derivative:\n",
    "\n",
    "$\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$\n",
    "\n",
    "Can be calculated using the quotient formula:\n",
    "\n",
    "![](images/sigmoid_derivative.gif)\n",
    "\n",
    "And if we have $m$ points labelled $x^{(1)}, x^{(2)}, ...,x^{(m)}$ , error formula is:\n",
    "\n",
    "$ E = -\\frac{1}{m}\\sum_{i=1}^m(y_i ln(\\hat{y_i})+(1-y_i)ln(1-\\hat{y_i})) $\n",
    "\n",
    "where the prediction is given by $\\hat{y_i} = \\sigma(Wx^{(i)}+b)$ \n",
    "\n",
    "The goal is to calculate the gradient of $E$, at a point $x = (x_1,...,x_n)$, given by the partial derivatives\n",
    "\n",
    "$\\nabla E = (\\frac{\\partial}{\\partial w_1}E, ..., \\frac{\\partial}{\\partial w_n}E, \\frac{\\partial}{\\partial b}E)$\n",
    "\n",
    "\n",
    "To simplify our calculations, we'll actually think of the error that each point produces, and calculate the derivative of this error. The total erro, then, is the average of the errors at all points. The error produced by each point is, simply,\n",
    "\n",
    "$E= -y ln(\\hat{y})-(1-y)ln(1-\\hat{y})$\n",
    "\n",
    "In order to calculate the derivative of this error with respect to the weights, we'll first calculate $\\frac{\\partial}{\\partial w_j}\\hat{y}$\n",
    "\n",
    "Recall that $\\hat{y} = \\sigma(Wx+b)$, so:\n",
    "\n",
    "\n",
    "$\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial w_j}\\hat{y} & = \\frac{\\partial}{\\partial w_j}\\sigma(Wx+b) \\\\\n",
    " & = \\sigma(Wx+b)(1-\\sigma(Wx+b)) \\cdot \\frac{\\partial}{\\partial W_j}(Wx+b) \\\\\n",
    " & = \\hat{y}(1-\\hat{y}) \\cdot \\frac{\\partial}{\\partial w_j}(Wx+b) \\\\\n",
    " & = \\hat{y}(1-\\hat{y}) \\cdot \\frac{\\partial}{\\partial w_j} (w_1x_1 + ... + w_jx_j + ... + w_nx_n + b) \\\\\n",
    " & = \\hat{y}(1-\\hat{y}) \\cdot x_i\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "The last equality is because the only term in the sum which is not a constant with respect to $w_j$ is precisely $w_jx_j$, which clearly has derivative $x_j$\n",
    "\n",
    "Now we calculate the derivative of the error $E$ at a point $x$, with respect to the weight $w_j$\n",
    "\n",
    "![](images/derivative_E.png)\n",
    "\n",
    "A similat calculation will show us that:\n",
    "\n",
    "$\\frac{\\partial}{\\partial b}E = -(y-\\hat{y})$\n",
    "\n",
    "This is demotrating something really important:\n",
    "For a point with coordinates $(x_1, ... , x_n)$, label $y$, and prediction $\\hat{y}$, the gradient of the error function at that point is $(-(y-\\hat{y})x_1, ..., -(y-\\hat{y})x_n, -(y-\\hat{y}))$\n",
    "\n",
    "In summary, the gradient is $\\nabla E = -(y-\\hat{y})(x_1, ...,x_n, 1)$\n",
    "\n",
    "`The gradient is actually a scalar times the coordinates of the point!`\n",
    "\n",
    "The scalar obtained signify:\n",
    "* Closer the label to the prediction, smaller the gradient.\n",
    "* Farther the label from the prediction, larger the gradient.\n",
    "\n",
    "A small gradient means we'll change our coordinates by a little bit, and a large gradient means we'll change our coordinates by a lot. (This sound like the perceptron algorithm)\n",
    "\n",
    "#### Gradient descent step\n",
    "Therefore, since the gradient descent step simply consists in subtracking a multiple of gradient of the error function at every point, then this updates the weight in the following way:\n",
    "\n",
    "$w'_i \\leftarrow w_i - \\alpha[-(y-\\hat{y})x_i]$,\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$w'_i \\leftarrow w_i + \\alpha(y-\\hat{y})x_i$.\n",
    "\n",
    "Similarly, it updates the bias in the following way:\n",
    "\n",
    "$b' \\leftarrow b + \\alpha(y-\\hat{y})$,\n",
    "\n",
    "Note: Since we've taken the average of the errors, the term we are adding should be $\\frac{1}{m} \\cdot \\alpha$ instead of $\\alpha$,but as $\\alpha$ is a constant, then in order to simplify calculations, we'll just take $\\frac{1}{m} \\cdot \\alpha$ to be our learning rate, and abuse the notation by just calling it $\\alpha$\n",
    "\n",
    "#### Gradient Descent Algorithm\n",
    "\n",
    "![](images/gradient_descent_algorithm.png)\n",
    "\n",
    "#### Perceptron vs Gradient Descent\n",
    "![](images/gradient_descent_algorithm_1.png)\n",
    "\n",
    "In gradient Descent Algorithm,\n",
    "![](images/gradient_descent_algorithm_2.png)\n",
    "The misclassified points ask to the line to come closer and the correctly classified points ask the line to go farther away, this make sense because  \n",
    "\n",
    "#### Continuous Perceptrons\n",
    "\n",
    "![](images/continuous_perceptrons_1.png)\n",
    "This gives us the probability function that looks like this, where the points on the blue or positive region have more chance of being blue and the points in the red or negative region have more chance of being red. \n",
    "\n",
    "And this give rise to this perceptron where we label the edges by the weights and the node by the bias.\n",
    "\n",
    "\n",
    "![](images/continuous_perceptrons_2.png)\n",
    "\n",
    "What the perceptron does, it takes 2 points, $x_1, x_2$ and plot in in the graph and then it returns a probability that the point is blue.\n",
    "\n",
    "#### Non-linear Data\n",
    "This curve that separate the regions is a set of points which are equally likely to be blue or red.\n",
    "\n",
    "![](images/non-lineal_regions.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
